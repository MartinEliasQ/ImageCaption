{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Importamos las funciones necesarias para correr los modelos\n",
    "from icg.processing import (text, image, dataset)\n",
    "from icg.model import model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"width: 100%;text-align:center;\"> Generación de subtítulos  de imagen usando aprendizaje profundo.  </h1>\n",
    "<p style=\"width: 100%;text-align:center;\">  Universidad de Antioquia <br> Angelower Santana Velasquez <br> Martin Elias Quintero  Osorio</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Las redes neuronales convolucionales representan actualmente la tecnologia por defecto en el reconocimiento de imagenes. Al realizar un paralelo para las estrategias usadas en el aprendizaje profundo para el reconocimiento de secuencias apareceria el nombre de las redes neuronales recurrentes. Estas ultimas, logran como resultado generalizar informacion temporal (secuencial) que se encuentra representada en los datos. Entre la forma de los datos secuenciales se encuentra: Series de tiempo, texto, voz, etc..</p>\n",
    "\n",
    "<p>Si bien, por un lado aprovechamos el potencial de las redes ocultas convolucionales para obtenerar una representacion de las caracteristicas de las imagenes, por otro, la informacion temporal es almacenada en cada unidad neuronal de las redes recurrentes. Cada una por su lado han demostrado la capacidad. El presente trabajo busca explorar una tecnica que combina ambas arquitecturas y realizar distintos experimentos donde se intentara mejorar el rendimiento de la arquitectura final.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"width: 100%;text-align:center;\"> El problema de la generacion de subtitulos </h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Consideremos la siguiente imagen.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"https://www.gopetplan.com/sites/default/files/2018-08/rsz_shutterstock_261121304.jpg\" alt=\"Drawing\" style=\"width: 100px;\"> <p style=\"width: 100%;text-align:center;\"><b><i>imagen 1 </i></b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Es probable que al rntentar realizar predicciones de la <b><i>Imagen 1</i></b> en una red convolucional la respuesta \n",
    "sea <b>perro</b>. Otras opciones de posible respuesta puede ser <b>cachorro</b> o con un nivel de probabilidad mucho menor <b>helado</b>. Este es un problema clásico de clasificación. Internamente la CNN es capaz de producir una representación de la imagen(extracción de características) y desde allí identificar la clase a la que pertenece dando valores de probabilidad a una cantidad definida de neuronas de salida(normalmente una capa softmax).</p>\n",
    "\n",
    "<p> Otra respuesta que es interesante seria lograr que la salida de la red logre decirnos las acciones que ocurren en la imagen, es decir, en vez de decirnos <b>perro</b> ,<b>cachorro</b> o <b>helado</b> la respuesta sera : <b>Cachorros comiendo helado</b>, <b>Dos cachorros lamiendo un helado de vainilla</b> o <b>Dos cachorros sobre una mesa de colores</b>.A este tipo de problemas se le conoce como <b>Image Caption Generation</b> o <b>Generación de subtítulos</b>           </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> La idea principal es utilizar la potencia del reconocimiento de las CNN con el desempeño de las RNN para generar texto.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"width: 100%;text-align:center;\"> Arquitecturas : Fusión e inyección </h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"width: 100%;text-align:center;\">Fusión </h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"./img/fusion.png\" alt=\"Drawing\" style=\"width: 100px;\"> <p style=\"width: 100%;text-align:center;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener una representación lingüística de la frase de entrada realizamos un procesamiento a priori mediante técnicas de procesamiento de lenguaje natural (Tokenizar, embeddings...), luego, usamos una red LSTM con n neuronas para lograr que el sistema adquiera la habilidad de reconocer secuencias. Paralelamente, para la imagen, modificamos las capas finales de una red convolución entrenada con 'imagenet', esta modificación es eliminar la capa softmax del final que es utilizada para detectar la clase con mayor probabilidad. Para las versiones 19 y 16 de VGG esto quiere decir que al final (luego de hacer el aplanamiento) obtenemos un vector con longitud 4096. Nuestro objetivo es combinar ambas representaciones, lingüística y visual, sumando los vectores  de las representaciones o uniendo uno tras el otro. Por ello, la red densa de la capa convolucional debe corresponder en tamaño (numero de neuronas) con la cantidad de neuronas de la LSTM. Finalmente pasamos la representación combinada por una capa softmax que aprenderá a discriminar la palabra mas probable. Las secuencias en el preprocesamiento cuentan con dos gatillos: Uno para iniciar una secuencia(< seqinit >) y otro para saber cuando se finaliza(< seqend >). La predicciónn de las secuencias es una a una. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"width: 100%;text-align:center;\"> Inyección </h1>\n",
    "\n",
    "Taken from: <a href=\"https://machinelearningmastery.com/caption-generation-inject-merge-architectures-encoder-decoder-model/\">Machine Learning Mastery</a>\n",
    "\n",
    "El modelo de inyección combina la forma codificada de la imagen con cada palabra de la descripción de texto generada hasta el momento.\n",
    "\n",
    "Este modelo de Inyección utiliza una red neuronal recurrente como modelo de generación de texto, que usa una secuencia de información de imagen y palabra como entrada para generar la siguiente palabra en la secuencia.\n",
    "\n",
    "En estas arquitecturas de \"inyección\", el vector de imagen (generalmente extraido de los valores de activación de una capa oculta en una red neuronal convolucional, en este caso la VGG) se inyecta en el RNN, por ejemplo, tratando el vector de imagen a la par con una \"palabra\" e incluyendo como parte del prefijo de subtítulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Inject-Architecture-for-Encoder-Decoder-Model.png\" alt=\"Drawing\" style=\"width: 80%;\"> \n",
    "<p style=\"width: 100%;text-align:center;\"><b><i>imagen 2 </i></b> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"width: 100%;text-align:center;\"> Preprocesamiento</h1>\n",
    "\n",
    "Un buen conjunto de datos para usar al comenzar con el subtitulado de imágenes es el conjunto de datos Flickr8K.\n",
    "\n",
    "El motivo es que es realista y relativamente pequeño para que pueda descargarlo y construir modelos en su estación de trabajo utilizando una CPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"width: 100%;text-align:center;\"> Texto</h1>\n",
    "\n",
    "Se debe hacer un pre-tratamiento del texto que contiene las descripciones de las imágenes. Se cuenta con un dataset con 8092 imágenes y un conjunto de 40460 descripciones de las imágenes, cada imagen tiene 5 frases que la describen (ver imagen 3). Cada frace inicia con el código de identificación de la imagen, el cual es el nombre del archivo de la imagen, esta información está contenida en un archivo txt, por lo que no hay que procesar caracteres especiales. \n",
    "\n",
    "<img style=\"float: center;\" src=\"./img/frases.png\" alt=\"Drawing\" style=\"width: 80%;\"> \n",
    "<p style=\"width: 100%;text-align:center;\"><b><i>imagen 3 </i></b> \n",
    "\n",
    "<ul> \n",
    "<li>La traducción del alemán original utiliza el inglés del Reino Unido (por ejemplo, \" viajando \").</li>\n",
    "<li>Las líneas están envueltas artificialmente con nuevas líneas de aproximadamente 70 caracteres (meh).</li>\n",
    "<li>No hay errores tipográficos obvios o errores ortográficos.</li>\n",
    "<li>Hay puntuaciones como comas, apóstrofes, comillas, signos de interrogación y más.</li>\n",
    "<li>Hay descripciones con guiones como \"como una armadura\".</li>\n",
    "<li>Se usan guiones (\"-\") para continuar las oraciones.</li>\n",
    "<li>Hay nombres de personas.</li>\n",
    "<li>No parece haber números que requieran manejo, como es el caso de años (2019)</li>\n",
    "    <li>Hay marcadores de sección (por ejemplo, \"II\" y \"III\").</li> \n",
    "    <li>Se ha eliminado el primer \"I\".</li>\n",
    "    </ul>\n",
    "    \n",
    "Los pasos a seguir para el pretratamiento del texto, se describen a continuación:\n",
    "<ol>\n",
    "    <li>Carga de datos</li>\n",
    "    <li>Hacer split por espacios en blanco.</li>\n",
    "    <li>Seleccionar palabras.</li>\n",
    "    <li>Eliminar puntuación.</li>\n",
    "    <li>Se vuelven minúsculas todas las letras.</li>\n",
    "</ol>\n",
    "\n",
    "Hay que destacar que aunque el pretratamiento del texto es una tarea muy pesada, para este caso en particular, el dataset ya está bien depurado, así que este proceso es reltivamente fácil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"width: 100%;text-align:center;\"> Imagen </h1>\n",
    "\n",
    "Se usan modelos pre-entrenado para extracción de características de las fotos. Hay muchos modelos para elegir. En este caso, utilizaremos los modelos de Oxford Visual Geometry Group, conocidas como VGG16 y VGG19. \n",
    "\n",
    "Como el interés de este trabajo no es la clasificación de las imágenes como lo hacer la VGG completa, se elimina la última capa del modelo y se extraen las características de la imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"width: 100%;text-align:center;\"> Experimentos </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Importamos las funciones necesarias para correr los modelos\n",
    "from icg.processing import (text, image, dataset)\n",
    "from icg.model import model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_TXT = \"../src/dataset/Flickr8k_text/Flickr8k.token.txt\"\n",
    "IMAGES = '../src/dataset/Flickr8k_Dataset/Flicker8k_Dataset'\n",
    "FEACTURES_PKL = \"../src/features.pkl\"\n",
    "\n",
    "URL_TEXT = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
    "URL_IMAGES =  \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n",
    "TRAIN_TXT = '../src/dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "TEXT_TXT = '../src/dataset/Flickr8k_text/Flickr_8k.testImages.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading : https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip in ../src/dataset\n",
      "It could take a few minutes\n",
      "Unzip... ../src/dataset/Flickr8k_Dataset.zip in ../src/dataset/Flickr8k_Dataset \n",
      "Downloading : https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip in ../src/dataset\n",
      "It could take a few minutes\n",
      "Unzip... ../src/dataset/Flickr8k_text.zip in ../src/dataset/Flickr8k_text \n"
     ]
    }
   ],
   "source": [
    "dataset.get_dataset(URL_IMAGES, URL_TEXT, dest=\"../src/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/martin/anaconda3/envs/martin/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "=================================================================\n",
      "Total params: 122,788,928\n",
      "Trainable params: 122,788,928\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Extracted Features: 8091\n"
     ]
    }
   ],
   "source": [
    "# Preprocesando imagenes\n",
    "# extract features from all images\n",
    "K.clear_session()\n",
    "features = image.extract_features(IMAGES)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "image.save_feactures(features,FEACTURES_PKL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "# Preprocesando el texto \n",
    "# load descriptions\n",
    "doc = text.load_doc(TOKEN_TXT)\n",
    "# parse descriptions\n",
    "descriptions = text.load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "text.clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = text.to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "text.save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "train = text.load_set(TRAIN_TXT)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = text.load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = image.load_photo_features(FEACTURES_PKL, train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos mediante el modelo de  fusión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 34, 256)      1940224     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 4096)         0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 34, 256)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          1048832     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          525312      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256)          0           dense_4[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 7579)         1947803     dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 221s 37ms/step - loss: 4.7915\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 220s 37ms/step - loss: 4.0550\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 220s 37ms/step - loss: 3.8137\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 220s 37ms/step - loss: 3.6589\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 220s 37ms/step - loss: 3.5699\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "tokenizer = text.create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = text.max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "# define the model\n",
    "tmp_model = model.define_model(vocab_size, max_length)\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 5\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "    # create the data generator\n",
    "    generator = text.data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "    # fit for one epoch\n",
    "    tmp_model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # save model\n",
    "    tmp_model.save('fmodel_' + str(i) + '.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "BLEU-1: 0.547386\n",
      "BLEU-2: 0.274671\n",
      "BLEU-3: 0.179011\n",
      "BLEU-4: 0.075552\n"
     ]
    }
   ],
   "source": [
    "# load test set\n",
    "test = text.load_set(TEXT_TXT)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = text.load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = image.load_photo_features(FEACTURES_PKL, test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "# load the model\n",
    "filename = 'fmodel_4.h5'\n",
    "my_model = load_model(filename)\n",
    "# evaluate model\n",
    "model.evaluate_model(my_model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos mediante el modelo de inyección\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 34, 256)      1940224     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 4096)         0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 34, 256)      0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          1048832     dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 256)          525312      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 256)          0           dense_7[0][0]                    \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          65792       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 7579)         1947803     dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 224s 37ms/step - loss: 4.8031\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 222s 37ms/step - loss: 4.0469\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 222s 37ms/step - loss: 3.8109\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 222s 37ms/step - loss: 3.6559\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 222s 37ms/step - loss: 3.5716\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "tokenizer = text.create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = text.max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "# define the model\n",
    "tmp_model = model.define_model(vocab_size, max_length)\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 5\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "    # create the data generator\n",
    "    generator = text.data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "    # fit for one epoch\n",
    "    tmp_model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # save model\n",
    "    tmp_model.save('imodel_' + str(i) + '.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "BLEU-1: 0.546388\n",
      "BLEU-2: 0.279281\n",
      "BLEU-3: 0.175566\n",
      "BLEU-4: 0.070699\n"
     ]
    }
   ],
   "source": [
    "# load test set\n",
    "test = text.load_set(TEXT_TXT)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = text.load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = image.load_photo_features(FEACTURES_PKL, test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "# load the model\n",
    "filename = 'imodel_4.h5'\n",
    "my_model = load_model(filename)\n",
    "# evaluate model\n",
    "model.evaluate_model(my_model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "martin",
   "language": "python",
   "name": "martin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
